{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41da386e",
   "metadata": {
    "papermill": {
     "duration": 0.002479,
     "end_time": "2024-11-11T20:47:20.320431",
     "exception": false,
     "start_time": "2024-11-11T20:47:20.317952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Phishing Email Detection using NLP and Machine Learning\n",
    "\n",
    "This project is designed to detect phishing emails by analyzing the text content using Natural Language Processing (NLP) and Machine Learning (ML). This project will help me understand how to preprocess email text, extract meaningful features, and build a simple classifier to differentiate phishing emails from legitimate ones.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Coming into this project, I faced a few challenges that I needed to address. First, I had limited experience with NLP and text preprocessing, so understanding how to clean and prepare raw email data was initially overwhelming. I also needed to familiarize myself with evaluation metrics like precision, recall, and F1-score. Lastly, understanding how to use TF-IDF for feature extraction was a new concept that took some time to grasp.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "1. **Set Up the Environment**: Install Python and essential libraries.\n",
    "2. **Gather the Dataset**: Use a dataset containing phishing and non-phishing emails (in this case the Spam vs Ham dataset).\n",
    "3. **Data Preprocessing**: Clean and preprocess the email text data.\n",
    "4. **Feature Engineering**: Transform text data into numerical features that the model can process.\n",
    "5. **Model Training**: Train a machine learning model to classify emails.\n",
    "6. **Evaluation**: Evaluate model performance using accuracy, precision, recall, and F1-score.\n",
    "7. **Testing and Improvement**: Test the model on new samples and improve if necessary.\n",
    "\n",
    "## Detailed Steps\n",
    "\n",
    "### 1. Set Up the Environment\n",
    "\n",
    "To set up the environment, ensure you have Python installed. Then, install the required libraries by running the following commands:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn nltk kagglehub\n",
    "```\n",
    "\n",
    "**Libraries Used**\n",
    "1. Pandas: For data manipulation and analysis.\n",
    "2. NumPy: For handling numerical operations.\n",
    "3. Scikit-learn: For machine learning algorithms, data splitting, and model evaluation.\n",
    "4. NLTK (Natural Language Toolkit): For text processing, tokenization, and removing stopwords.\n",
    "    - Stopwords: common words that typically do not carry significant meaning (i.e. 'the', 'in' 'is')\n",
    "    - punkt: downloads the tokenizer models which helps split text into words or sentences based on standard english\n",
    "4. KaggleHub: For downloading datasets from Kaggle directly into the project.\n",
    "\n",
    "### 2. Downloading and Accessing the Dataset\n",
    "The dataset is downloaded using the kagglehub library. We specify the spam vs ham dataset and print out the path to where the dataset is saved on the system.\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"shantanudhakadd/email-spam-detection-dataset-classification\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "```\n",
    "\n",
    "Next, the dataset is loaded into a pandas DataFrame for easier manipulation and analysis. We inspect the first few rows to understand its structure.\n",
    "\n",
    "```python\n",
    "data = pd.read_csv(f\"{path}/emails.csv\") \n",
    "print(data.head())\n",
    "```\n",
    "### 5. Data Preprocessing\n",
    "We preprocess the email text data to prepare it for model training focsuing on:\n",
    "    - Convert text to lowercase.\n",
    "    - Remove special characters and numbers.\n",
    "    - Tokenize the text and remove stopwords using NLTK.\n",
    "    - Store the cleaned text in a new column, processed_text.\n",
    "\n",
    "``` python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['processed_text'] = data['message'].apply(preprocess_text)\n",
    "```\n",
    "\n",
    "### 6. Feature Engineering\n",
    "We convert the processed text data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency), limiting it to the top 3000 features for simplicity. This transforms the text into a format that the model can work with. TF_IFG converts text into numerical values representing the importance of words in the text. The labels (y) are also transformed into binary values for classification.\n",
    "\n",
    "```\n",
    "python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf_vectorizer.fit_transform(data['processed_text']).toarray()\n",
    "y = data['label'].apply(lambda x: 1 if x == 'phishing' else 0)  # Convert labels to binary\n",
    "```\n",
    "\n",
    "The lambda functions operates as follows:\n",
    "1. For each label x in the label column:\n",
    "2. If x is equal to 'phishing', it returns 1.\n",
    "    - Otherwise (if x is not 'phishing', like 'not phishing'), it returns 0.\n",
    "This effectively converts any 'phishing' label to 1 and any other label to 0.\n",
    "Y is now a binary representation of the label column where 1 represents phishing emails and 0 non-phishing emails. This binary classification prepares y to be used as a target variable in a ML model.\n",
    "\n",
    "### 7. Splitting the Dataset\n",
    "We split the dataset into training and testing sets, with 80% for training and 20% for testing. This helps evaluate the model's performance on unseen data.\n",
    "\n",
    "```\n",
    "python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### 8. Model Training\n",
    "We use a Naive Bayes classifier, which is effective for text classification tasks and perfect for identifying phishing emails. The model is trained on the training data.\n",
    "\n",
    "```\n",
    "python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 9. Making Predictions and Evaluation\n",
    "After training the model, we use it to make predictions on the test set. We then calculate evaluation metrics, such as accuracy, precision, recall, and F1-score, to assess model performance.\n",
    "\n",
    "```\n",
    "python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "```\n",
    "\n",
    "\n",
    "## What I Learned\n",
    "Through this project, I learned how to preprocess text data for NLP tasks, including steps like tokenization, stopword removal, and lowercasing, which are crucial for cleaning raw text data. Working with TF-IDF taught me how to transform text into numerical features that machine learning models can understand, which was eye-opening. I also gained a deeper understanding of model evaluation metrics beyond accuracy, such as precision, recall, and F1-score, and how each metric is valuable in understanding the performance of a classification model. Finally, I learned how to use libraries like NLTK and scikit-learn together to build an end-to-end machine learning pipeline for text classification, which has given me more confidence in my ability to work on similar NLP projects in the future.\n",
    "\n",
    "1. Python re.sub(): search, replace, and manipulate strings based on specific patterns\n",
    "    - re.sub(pattern, replacement, string)\n",
    "    - i.e. `text = re.sub(r'\\W', ' ', text)\n",
    "        - \\W matche any non-word character \n",
    "        - removes any symbols or special characters \n",
    "    - the r before a string makes it a raw string, telling python to treat \\ as literal characters\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "This project guides you through the end-to-end process of building a phishing email detection model, from data preprocessing to model training and evaluation. By following these steps, you can understand the basics of text classification, feature engineering with TF-IDF, and model evaluation using essential metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1c2da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T20:47:20.326631Z",
     "iopub.status.busy": "2024-11-11T20:47:20.326121Z",
     "iopub.status.idle": "2024-11-11T20:47:25.997538Z",
     "shell.execute_reply": "2024-11-11T20:47:25.994243Z"
    },
    "papermill": {
     "duration": 5.678663,
     "end_time": "2024-11-11T20:47:26.001219",
     "exception": false,
     "start_time": "2024-11-11T20:47:20.322556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/email-spam-detection-dataset-classification\n",
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.98\n",
      "Precision: 0.99\n",
      "Recall: 0.85\n",
      "F1 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download and access dataset from online kaggle api\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"shantanudhakadd/email-spam-detection-dataset-classification\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Set up dataset path\n",
    "dataset_path = f\"{path}/spam.csv\" \n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(dataset_path, encoding='ISO-8859-1')\n",
    "\n",
    "\n",
    "# Inspect dataset\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Assigns all stop words to a set for easy lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to email body column to create new column 'processed_text'\n",
    "data['processed_text'] = data['v2'].apply(preprocess_text)  \n",
    "\n",
    "# Feature Engineering with TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf_vectorizer.fit_transform(data['processed_text']).toarray()\n",
    "y = data['v1'].apply(lambda x: 1 if x == 'spam' else 0)  # Convert 'spam' to 1 and 'ham' to 0\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training with multinomial naive bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be12374",
   "metadata": {
    "papermill": {
     "duration": 0.003219,
     "end_time": "2024-11-11T20:47:26.008211",
     "exception": false,
     "start_time": "2024-11-11T20:47:26.004992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1961542,
     "sourceId": 3235802,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.298646,
   "end_time": "2024-11-11T20:47:26.734909",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-11T20:47:17.436263",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
